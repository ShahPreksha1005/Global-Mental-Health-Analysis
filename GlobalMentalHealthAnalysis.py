# -*- coding: utf-8 -*-
"""Lab4_2348446.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WA5DqrgsI1T8cRGp35ILVRy5eYx_x0SH

# **Lab 4 - Principal Component Analysis**

**Created By: Preksha Shah**

**Date: 09.03.2024**

---

# **Import the libraries**

---
"""

# Step 1: Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

"""# **Load the dataset**

---


"""

# Step 2: Load the dataset
df = pd.read_csv("/content/1- mental-illnesses-prevalence.csv")

"""# **Data exploration**

---


"""

# Step 3: Data Exploration
print("Data Exploration:")
df.head()  # Display the first few rows of the dataset

df.info()   # Display information about the dataset, including data types and missing values

"""# **Univariate Analysis**

---


"""

# Step 4: Univariate Analysis

# a. Calculate basic descriptive statistics
print("Basic Descriptive Statistics:")
df.describe()

import plotly.express as px

# Select numerical columns for visualization
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Create a separate histogram for each numerical variable
for col in numerical_cols:
    fig = px.histogram(df, x=col, title=f'Distribution of {col}')
    fig.update_layout(xaxis_title='Share of Population', yaxis_title='Frequency')
    fig.show()

import plotly.figure_factory as ff

# Select numerical columns for visualization
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Create a separate kernel density plot for each numerical variable
for col in numerical_cols:
    fig = ff.create_distplot([df[col]], [col], bin_size=0.25, show_rug=False)
    fig.update_layout(title=f'Kernel Density Plot of {col}', xaxis_title='Value', yaxis_title='Density')
    fig.show()

import plotly.express as px

# Select numerical columns for visualization
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Create a separate box plot for each numerical variable
for col in numerical_cols:
    fig = px.box(df, y=col, title=f'Box Plot of {col}')
    fig.update_layout(yaxis_title='Value')
    fig.show()

# Step 1: Display frequency tables showing counts and percentages for categorical variables
# Let's assume 'Entity' and 'Code' are the categorical variables in this case
categorical_cols = ['Entity', 'Code']

for col in categorical_cols:
    print(f"Frequency table for {col}:")
    print(df[col].value_counts())
    print("\nPercentage table for {col}:")
    print(df[col].value_counts(normalize=True) * 100)
    print("\n")

# Step 2: Visualize using bar plots for categorical variables
import plotly.express as px

for col in categorical_cols:
    fig = px.bar(df[col].value_counts(), x=df[col].value_counts().index, y=df[col].value_counts().values,
                 title=f'Bar Plot of {col}', labels={col: 'Category', 'value': 'Count'})
    fig.show()

"""# **Bivariate Analysis**

---


"""

df.columns

import plotly.graph_objects as go

# Define the size of each subplot
subplot_size = 1000

# Define the pairs of numerical variables for scatter plots
scatter_pairs = [
    (       'Schizophrenia disorders (share of population) - Sex: Both - Age: Age-standardized',       'Depressive disorders (share of population) - Sex: Both - Age: Age-standardized'),
    (      'Schizophrenia disorders (share of population) - Sex: Both - Age: Age-standardized', 'Anxiety disorders (share of population) - Sex: Both - Age: Age-standardized'),
    ('Depressive disorders (share of population) - Sex: Both - Age: Age-standardized', 'Anxiety disorders (share of population) - Sex: Both - Age: Age-standardized')
]

# Create scatter plots for selected pairs of numerical variables
for col1, col2 in scatter_pairs:
    fig = go.Figure(data=go.Scatter(x=df[col1], y=df[col2], mode='markers'))
    fig.update_layout(title=f'Scatter Plot of {col1} vs {col2}', width=subplot_size, height=subplot_size)
    fig.show()

import plotly.graph_objects as go

# Define the size of each subplot
subplot_size = 800  # Adjust this value as needed for clarity

# Create individual box plots for each numerical variable grouped by 'Entity'
for col in numerical_cols:
    fig = px.box(df, x='Entity', y=col, title=f'Box Plot of {col} by Entity')
    fig.update_layout(width=subplot_size, height=subplot_size, xaxis_title='Entity', yaxis_title=col, title_x=0.5)
    fig.show()

import plotly.graph_objects as go

# Define the size of each subplot
subplot_size = 800  # Adjust this value as needed for clarity

# Create individual violin plots for each numerical variable grouped by 'Code'
for col in numerical_cols:
    fig = px.violin(df, x='Code', y=col, title=f'Violin Plot of {col} by Code')
    fig.update_layout(width=subplot_size, height=subplot_size, xaxis_title='Code', yaxis_title=col, title_x=0.5)
    fig.show()

import plotly.graph_objects as go

# Define the size of the heatmap
heatmap_width = 800
heatmap_height = 800

# Calculate correlation matrix
correlation_matrix = df[numerical_cols].corr()

# Shorten column names for better display on the heatmap
short_labels = [col[:10] for col in correlation_matrix.columns]

# Plot correlation matrix as a heatmap with shortened labels
fig = px.imshow(correlation_matrix, color_continuous_scale='Viridis',
                title='Correlation Heatmap of Numerical Variables',
                labels=dict(x='Variables', y='Variables', color='Correlation'))
fig.update_xaxes(tickvals=list(range(len(short_labels))), ticktext=short_labels)
fig.update_yaxes(tickvals=list(range(len(short_labels))), ticktext=short_labels)
fig.update_layout(width=heatmap_width, height=heatmap_height)
fig.show()

df

"""# **Principal Component Analysis - Conventional Method**

---


"""

from sklearn.preprocessing import StandardScaler

# Select numerical columns
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Standardize numerical columns
scaler = StandardScaler()
df_std_numeric = scaler.fit_transform(df[numerical_cols])

import seaborn as sns
# Compute mean vector
mean_vec = np.mean(df_std_numeric, axis=0)

# Compute covariance matrix
cov_mat = (df_std_numeric - mean_vec).T.dot((df_std_numeric - mean_vec)) / (df_std_numeric.shape[0] - 1)
print('Covariance matrix:\n', cov_mat)

# Alternatively, you can use np.cov
cov_mat_np = np.cov(df_std_numeric.T)
print('NumPy covariance matrix:\n', cov_mat_np)

# Check the correlation between different features
plt.figure(figsize=(8, 8))
sns.heatmap(cov_mat, vmax=1, square=True, annot=True, cmap='cubehelix')
plt.title('Correlation between different features')
plt.show()

# Eigen decomposition of the covariance matrix
eig_vals, eig_vecs = np.linalg.eig(cov_mat)
print('Eigenvectors:\n', eig_vecs)
print('\nEigenvalues:\n', eig_vals)

# Make a list of (eigenvalue, eigenvector) tuples
eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]

# Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs.sort(key=lambda x: x[0], reverse=True)

# Print eigenvalues in descending order
print('Eigenvalues in descending order:')
for i in eig_pairs:
    print(i[0])

# Explained Variance
tot = sum(eig_vals)
var_exp = [(i / tot) * 100 for i in sorted(eig_vals, reverse=True)]

# Plot explained variance ratio
with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))
    plt.bar(range(len(var_exp)), var_exp, alpha=0.5, align='center', label='individual explained variance')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()
    plt.show()

# Construct projection matrix
matrix_w = np.hstack((eig_pairs[0][1].reshape(-1, 1),
                      eig_pairs[1][1].reshape(-1, 1)))
print('Projection Matrix (W):\n', matrix_w)


# Projection onto the New Feature Space
Y = df_std_numeric.dot(matrix_w)
print('Projected Data (Y):\n', Y)
print('Shape of Projected Data (Y):', Y.shape)

"""# **Principal Component Analysis - Scikit-learn**

---


"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Initialize PCA and fit to standardized data
pca = PCA().fit(df_std_numeric)

# Plot cumulative explained variance
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.title('Cumulative Explained Variance Ratio')
plt.show()

# Perform PCA with specified number of components
sklearn_pca = PCA(n_components=5)
Y_sklearn = sklearn_pca.fit_transform(df_std_numeric)

# Output transformed data
print(Y_sklearn)
print("Transformed data shape:", Y_sklearn.shape)

"""# **Conclusion and Inference**

---

#### Data Exploration:
- The dataset consists of information on mental illnesses prevalence across different countries and years.
- It contains a total of 6420 records and 8 columns.
- The columns include features such as the entity, country code, year, and various mental disorder prevalence rates.
- Initial exploration suggests that the dataset contains both numerical and categorical variables.

#### Univariate Analysis:
- Basic descriptive statistics and visualization techniques were used to understand the distribution of numerical variables.
- Histograms, kernel density plots, and box plots were utilized to visualize the distribution, central tendency, and spread of each numerical variable.
- Frequency tables and bar plots were used to analyze categorical variables, providing insights into the occurrence of different categories within each variable.

#### Bivariate Analysis:
- Scatter plots were used to visualize relationships between pairs of numerical variables, revealing potential correlations or patterns.
- Box plots and violin plots were used to compare the distribution of numerical variables across different categories, such as entities or country codes.
- A correlation heatmap was generated to explore the pairwise correlations between numerical variables.

#### Principal Component Analysis (PCA):
- PCA was performed using both conventional methods and scikit-learn.
- The conventional method involved computing the covariance matrix, eigen decomposition, and selection of principal components.
- Scikit-learn's PCA module was used to analyze the explained variance ratio and perform dimensionality reduction.
- The number of principal components required to capture 90% of the variance was determined.

#### Overall:
- The dataset provides valuable insights into the prevalence of mental illnesses across countries and years.
- Univariate and bivariate analyses helped understand the distribution and relationships between variables.
- PCA was utilized for dimensionality reduction and identifying key components contributing to the variance in the dataset.
- The cumulative explained variance ratio was visualized to determine the appropriate number of components for dimensionality reduction.

These analyses provide a comprehensive understanding of the dataset and can guide further exploration or modeling tasks related to mental health prevalence.
"""